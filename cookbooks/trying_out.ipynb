{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Add the project root directory to the Python path\n",
    "import os\n",
    "import sys\n",
    "\n",
    "# Use the current working directory as the project root\n",
    "project_root = os.path.abspath('/Users/jan/Desktop/advanced_rag/')\n",
    "sys.path.insert(0, project_root)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Llama 2 1B:\n",
      "Response: Machine learning is a subset of artificial intelligence that enables computers to learn from data, make predictions or decisions without being explicitly programmed, by analyzing patterns and relationships within the data.\n",
      "Time taken: 2.76 seconds\n",
      "\n",
      "Llama 2 3B:\n",
      "Response: Machine learning is a subset of artificial intelligence that enables computers to automatically improve their performance on a specific task by learning from data and experience, without being explicitly programmed.\n",
      "Time taken: 3.19 seconds\n",
      "Phi-3.5:\n",
      "Response: Machine learning is a subset of artificial intelligence that uses algorithms and statistical models to enable computers to improve their performance on a specific task through experience, without being explicitly programmed for every contingency.\n",
      "\n",
      "In simpler terms: Machine Learning involves teaching machines how to learn from data so they can make decisions or predictions more accurately over time with minimal human intervention.\n",
      "Time taken: 5.61 seconds\n"
     ]
    }
   ],
   "source": [
    "import time\n",
    "from langchain_ollama import ChatOllama\n",
    "\n",
    "# Load Llama 2 1B model\n",
    "llama_1b = ChatOllama(model=\"llama3.2:1b\")\n",
    "\n",
    "# Load Llama 2 2B model\n",
    "llama_2b = ChatOllama(model=\"llama3.2\")\n",
    "\n",
    "phi_3_5 = ChatOllama(model=\"phi3.5\")\n",
    "\n",
    "# Test prompt\n",
    "test_prompt = \"Explain the concept of machine learning in one sentence.\"\n",
    "\n",
    "# Speed test function\n",
    "def speed_test(model, prompt):\n",
    "    start_time = time.time()\n",
    "    response = model.invoke(prompt)\n",
    "    end_time = time.time()\n",
    "    return response.content, end_time - start_time\n",
    "\n",
    "# Run speed test for 1B model\n",
    "response_1b, time_1b = speed_test(llama_1b, test_prompt)\n",
    "\n",
    "# Run speed test for 2B model\n",
    "response_2b, time_2b = speed_test(llama_2b, test_prompt)\n",
    "\n",
    "response_phi, time_phi = speed_test(phi_3_5, test_prompt)\n",
    "\n",
    "# Print results\n",
    "print(\"Llama 2 1B:\")\n",
    "print(f\"Response: {response_1b}\")\n",
    "print(f\"Time taken: {time_1b:.2f} seconds\\n\")\n",
    "\n",
    "print(\"Llama 2 3B:\")\n",
    "print(f\"Response: {response_2b}\")\n",
    "print(f\"Time taken: {time_2b:.2f} seconds\")\n",
    "\n",
    "print(\"Phi-3.5:\")\n",
    "print(f\"Response: {response_phi}\")\n",
    "print(f\"Time taken: {time_phi:.2f} seconds\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "advanced_rag",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
