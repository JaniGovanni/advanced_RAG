{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "import os\n",
    "from dotenv import load_dotenv\n",
    "\n",
    "current_dir = os.getcwd()\n",
    "env_path = os.path.abspath(os.path.join(current_dir, '..', 'app', '.env'))\n",
    "load_dotenv(env_path)\n",
    "\n",
    "project_root = os.path.abspath(os.path.join(current_dir, '..'))\n",
    "sys.path.append(project_root)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Function imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from app.vectorstore.embeddings import JinaEmbeddings\n",
    "from app.vectorstore import get_chroma_store_as_retriever, add_docs_to_store\n",
    "from app.vectorstore.experimental import get_faiss_store_as_retriever, add_docs_to_faiss_store\n",
    "from app.doc_processing.late_chunking import apply_late_chunking"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Berlin documet test"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This is the test, which the developers from Jina Ai ran in there colab notebook (https://colab.research.google.com/drive/15vNZb6AsU7byjYoaEtXuNu567JWNzXOz?usp=sharing) redone with my own implemented classes and functions, to implement the late chuking approach into the langchain framework.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here are some functions from the reference implementation. We use them later."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def chunk_by_sentences(input_text: str, tokenizer: callable):\n",
    "    \"\"\"\n",
    "    Split the input text into sentences using the tokenizer\n",
    "    :param input_text: The text snippet to split into sentences\n",
    "    :param tokenizer: The tokenizer to use\n",
    "    :return: A tuple containing the list of text chunks and their corresponding token spans\n",
    "    \"\"\"\n",
    "    inputs = tokenizer(input_text, return_tensors='pt', return_offsets_mapping=True)\n",
    "    punctuation_mark_id = tokenizer.convert_tokens_to_ids('.')\n",
    "    sep_id = tokenizer.convert_tokens_to_ids('[SEP]')\n",
    "    token_offsets = inputs['offset_mapping'][0]\n",
    "    token_ids = inputs['input_ids'][0]\n",
    "    chunk_positions = [\n",
    "        (i, int(start + 1))\n",
    "        for i, (token_id, (start, end)) in enumerate(zip(token_ids, token_offsets))\n",
    "        if token_id == punctuation_mark_id\n",
    "        and (\n",
    "            token_offsets[i + 1][0] - token_offsets[i][1] > 0\n",
    "            or token_ids[i + 1] == sep_id\n",
    "        )\n",
    "    ]\n",
    "    chunks = [\n",
    "        input_text[x[1] : y[1]]\n",
    "        for x, y in zip([(1, 0)] + chunk_positions[:-1], chunk_positions)\n",
    "    ]\n",
    "    span_annotations = [\n",
    "        (x[0], y[0]) for (x, y) in zip([(1, 0)] + chunk_positions[:-1], chunk_positions)\n",
    "    ]\n",
    "    return chunks, span_annotations\n",
    "\n",
    "def late_chunking_reference(\n",
    "    model_output: 'BatchEncoding', span_annotation: list, max_length=None\n",
    "):\n",
    "    token_embeddings = model_output[0]\n",
    "    outputs = []\n",
    "    for embeddings, annotations in zip(token_embeddings, span_annotation):\n",
    "        if (\n",
    "            max_length is not None\n",
    "        ):  # remove annotations which go bejond the max-length of the model\n",
    "            annotations = [\n",
    "                (start, min(end, max_length - 1))\n",
    "                for (start, end) in annotations\n",
    "                if start < (max_length - 1)\n",
    "            ]\n",
    "        pooled_embeddings = [\n",
    "            embeddings[start:end].sum(dim=0) / (end - start)\n",
    "            for start, end in annotations\n",
    "            if (end - start) >= 1\n",
    "        ]\n",
    "        pooled_embeddings = [\n",
    "            embedding.detach().cpu().numpy() for embedding in pooled_embeddings\n",
    "        ]\n",
    "        outputs.append(pooled_embeddings)\n",
    "\n",
    "    return outputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/jan/miniconda3/envs/advanced_rag/lib/python3.11/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "# test document\n",
    "input_text = \"Berlin is the capital and largest city of Germany, both by area and by population. Its more than 3.85 million inhabitants make it the European Union's most populous city, as measured by population within city limits. The city is also one of the states of Germany, and is the third smallest state in the country in terms of area.\"\n",
    "# langchain embedding class wrapper around huggingface tokenizer and model\n",
    "# note: i use the small model instead of the base model because of cpu limitations\n",
    "embeddings = JinaEmbeddings()\n",
    "sentences, span_annotations = chunk_by_sentences(input_text, embeddings.tokenizer)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we have to convert the list of sentences into a list of langchain documents,\n",
    "to use my functions from the module app.vectorstore."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.schema import Document\n",
    "docs = [Document(page_content=sentence, metadata={\"tag\": 'test_berlin', \"source\": \"berlin.txt\"}) for sentence in sentences]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Default approach, without late chunking. (Chunking -> Embedding -> Vector Store)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "chroma_retriever = get_chroma_store_as_retriever(embeddings=embeddings)\n",
    "add_docs_to_store(chroma_retriever, docs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "New late chunking approach. (Embedding -> Chunking -> Vector Store)\n",
    "\n",
    "Its nessecary to use a FAISS index here, because it is impossible to add already existing embeddings to a chroma store.\n",
    "At least i didn't found a way to do it.\n",
    "The FAISS index has a method called add_embeddings, which i have used here. You can see the implementation in the add_docs_to_faiss_store method."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Creating new FAISS index\n",
      "New FAISS index saved to app/data/FAISS_STORE\n",
      "FAISS index saved to app/data/FAISS_STORE\n"
     ]
    }
   ],
   "source": [
    "late_chunked_docs = apply_late_chunking(docs)\n",
    "faiss_retriever = get_faiss_store_as_retriever()\n",
    "add_docs_to_faiss_store(faiss_retriever, late_chunked_docs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now lets test the retrievers!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Chroma Results:\n",
      "Score: 33.3837, Content: Berlin is the capital and largest city of Germany, both by area and by population....\n",
      "Score: 50.2768, Content:  The city is also one of the states of Germany, and is the third smallest state in the country in te...\n",
      "Score: 60.6438, Content:  Its more than 3.85 million inhabitants make it the European Union's most populous city, as measured...\n",
      "\n",
      "FAISS Results:\n",
      "Score: 29.3441, Content: Berlin is the capital and largest city of Germany, both by area and by population....\n",
      "Score: 41.2074, Content:  Its more than 3.85 million inhabitants make it the European Union's most populous city, as measured...\n",
      "Score: 44.7700, Content:  The city is also one of the states of Germany, and is the third smallest state in the country in te...\n"
     ]
    }
   ],
   "source": [
    "query = \"Berlin\"\n",
    "\n",
    "chroma_results = chroma_retriever.vectorstore.similarity_search_with_score(query, k=3)\n",
    "faiss_results = faiss_retriever.vectorstore.similarity_search_with_score(query, k=3)\n",
    "\n",
    "\n",
    "print(\"Chroma Results:\")\n",
    "for doc, score in chroma_results:\n",
    "    print(f\"Score: {score:.4f}, Content: {doc.page_content[:100]}...\")\n",
    "\n",
    "print(\"\\nFAISS Results:\")\n",
    "for doc, score in faiss_results:\n",
    "    print(f\"Score: {score:.4f}, Content: {doc.page_content[:100]}...\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Hmm, That is quite underwhelming.\n",
    "I think its time to do some comparison tests with the reference implementation."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Tests"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here we do a little check, if the created wrapper for the jina embeddings model is working correctly, by compairing my implementation with the reference implementation from the jina authors."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The embedding similarity between the own implementation and the reference implementation is 0.9999999457983118 percent\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "from transformers import AutoModel\n",
    "from transformers import AutoTokenizer\n",
    "\n",
    "\n",
    "# load model and tokenizer\n",
    "tokenizer = AutoTokenizer.from_pretrained('jinaai/jina-embeddings-v2-small-en', trust_remote_code=True)\n",
    "model = AutoModel.from_pretrained('jinaai/jina-embeddings-v2-small-en', trust_remote_code=True)\n",
    "\n",
    "berlin_embedding_ref = model.encode('Berlin')\n",
    "berlin_embedding_own = embeddings.embed_query(\"Berlin\")\n",
    "\n",
    "cos_sim = lambda x, y: np.dot(x, y) / (np.linalg.norm(x) * np.linalg.norm(y))\n",
    "\n",
    "print(f'The embedding similarity between the own implementation and the reference implementation is {cos_sim(berlin_embedding_ref, berlin_embedding_own)} percent')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Alright. That looks quit good so far. Lets check the late chunking implementation. We have already calculated the span_annotations\n",
    "with the function above (chunk_by_sentences)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "inputs = tokenizer(input_text, return_tensors='pt')\n",
    "model_output = model(**inputs)\n",
    "embeddings_ref = late_chunking_reference(model_output, [span_annotations])[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Lets compare them."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sentence 1 similarity: 0.9997\n",
      "Sentence 2 similarity: 1.0000\n",
      "Sentence 3 similarity: 1.0000\n"
     ]
    }
   ],
   "source": [
    "late_chunked_embeddings = [doc.metadata['embedding'] for doc in late_chunked_docs]\n",
    "\n",
    "# Compare embeddings\n",
    "for i, (ref_embedding, own_embedding) in enumerate(zip(embeddings_ref, late_chunked_embeddings)):\n",
    "    similarity = cos_sim(ref_embedding, own_embedding)\n",
    "    print(f\"Sentence {i+1} similarity: {similarity:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "They look also quit good, so i suggest my implementation works fine! So it might be like the following:\n",
    "\n",
    "* The cosine similarity between late chunked embeddings and the search query is actually higher than the similarity between the regular embedded chunks and the query.\n",
    "\n",
    "* That seems to be irrelevant for the langchain framework, because the vectorstores dont seems to use the cosine similarity, to search related chunks. (Based on my observations in this notebook and my runned test it looks like it)\n",
    "\n",
    "**Conclusion**\n",
    "\n",
    "A custom vectorstore object is needed, which uses the cosine similariy to determine, if documents are relevant for the search query"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Implementing custom similarity search for vectorstore object"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Do it again with the custom implementation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "faiss_retriever = get_faiss_store_as_retriever(custom=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Chroma Results:\n",
      "Cosine Similarity: 0.8802, Content: Berlin is the capital and largest city of Germany, both by area and by population....\n",
      "Cosine Similarity: 0.7919, Content:  The city is also one of the states of Germany, and is the third smallest state in the country in te...\n",
      "Cosine Similarity: 0.7413, Content:  Its more than 3.85 million inhabitants make it the European Union's most populous city, as measured...\n",
      "\n",
      "FAISS Results:\n",
      "Cosine Similarity: 0.8857, Content: Berlin is the capital and largest city of Germany, both by area and by population....\n",
      "Cosine Similarity: 0.8516, Content:  The city is also one of the states of Germany, and is the third smallest state in the country in te...\n",
      "Cosine Similarity: 0.8478, Content:  Its more than 3.85 million inhabitants make it the European Union's most populous city, as measured...\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "query = \"Berlin\"\n",
    "\n",
    "# Calculate query embedding\n",
    "query_embedding = embeddings.embed_query(query)\n",
    "\n",
    "chroma_results = chroma_retriever.vectorstore.similarity_search_with_score(query, k=3)\n",
    "\n",
    "# new implemented method, is also called when called similarity_search\n",
    "faiss_results = faiss_retriever.vectorstore.similarity_search_by_cosine(query, k=3)\n",
    "\n",
    "def cos_sim(x, y):\n",
    "    return np.dot(x, y) / (np.linalg.norm(x) * np.linalg.norm(y))\n",
    "\n",
    "print(\"Chroma Results:\")\n",
    "for doc, score in chroma_results:\n",
    "    doc_embedding = embeddings.embed_documents([doc.page_content])[0]\n",
    "    cosine_similarity = cos_sim(query_embedding, doc_embedding)\n",
    "    print(f\"Cosine Similarity: {cosine_similarity:.4f}, Content: {doc.page_content[:100]}...\")\n",
    "\n",
    "print(\"\\nFAISS Results:\")\n",
    "for doc, score in faiss_results:\n",
    "    print(f\"Cosine Similarity: {score:.4f}, Content: {doc.page_content[:100]}...\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Note:**\n",
    "\n",
    "The custom implementation doesnt actually uses the cosine similarity, to search for the most similar chunks. That is, because it seems to be not easy to overwrite the actual search method from the vectorstore. It presearches for oversampling_factor * k similar documents, with the default search method, rerank them based on the cosine similaritys and returns the k documents with the highest similaritys."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "advanced_rag",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
