{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "import os\n",
    "from dotenv import load_dotenv\n",
    "\n",
    "load_dotenv()\n",
    "\n",
    "# Get the current working directory\n",
    "current_dir = os.getcwd()\n",
    "\n",
    "# Assuming your notebook is in a subdirectory of the main project folder,\n",
    "# go up two levels to reach the project root\n",
    "project_root = os.path.abspath(os.path.join(current_dir, '..', '..', '..'))\n",
    "sys.path.append(project_root)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## reference implementation\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Transformers cache directory: /Users/jan/.cache/huggingface/hub\n"
     ]
    }
   ],
   "source": [
    "from transformers.utils import TRANSFORMERS_CACHE\n",
    "print(f\"Transformers cache directory: {TRANSFORMERS_CACHE}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def late_chunking(\n",
    "    model_output: 'BatchEncoding', span_annotation: list, max_length=None\n",
    "):\n",
    "    token_embeddings = model_output[0]\n",
    "    outputs = []\n",
    "    for embeddings, annotations in zip(token_embeddings, span_annotation):\n",
    "        if (\n",
    "            max_length is not None\n",
    "        ):  # remove annotations which go bejond the max-length of the model\n",
    "            annotations = [\n",
    "                (start, min(end, max_length - 1))\n",
    "                for (start, end) in annotations\n",
    "                if start < (max_length - 1)\n",
    "            ]\n",
    "        pooled_embeddings = [\n",
    "            embeddings[start:end].sum(dim=0) / (end - start)\n",
    "            for start, end in annotations\n",
    "            if (end - start) >= 1\n",
    "        ]\n",
    "        pooled_embeddings = [\n",
    "            embedding.detach().cpu().numpy() for embedding in pooled_embeddings\n",
    "        ]\n",
    "        outputs.append(pooled_embeddings)\n",
    "\n",
    "    return outputs\n",
    "\n",
    "def chunk_by_sentences(input_text: str, tokenizer: callable):\n",
    "    \"\"\"\n",
    "    Split the input text into sentences using the tokenizer\n",
    "    :param input_text: The text snippet to split into sentences\n",
    "    :param tokenizer: The tokenizer to use\n",
    "    :return: A tuple containing the list of text chunks and their corresponding token spans\n",
    "    \"\"\"\n",
    "    inputs = tokenizer(input_text, return_tensors='pt', return_offsets_mapping=True)\n",
    "    punctuation_mark_id = tokenizer.convert_tokens_to_ids('.')\n",
    "    sep_id = tokenizer.convert_tokens_to_ids('[SEP]')\n",
    "    token_offsets = inputs['offset_mapping'][0]\n",
    "    token_ids = inputs['input_ids'][0]\n",
    "    chunk_positions = [\n",
    "        (i, int(start + 1))\n",
    "        for i, (token_id, (start, end)) in enumerate(zip(token_ids, token_offsets))\n",
    "        if token_id == punctuation_mark_id\n",
    "        and (\n",
    "            token_offsets[i + 1][0] - token_offsets[i][1] > 0\n",
    "            or token_ids[i + 1] == sep_id\n",
    "        )\n",
    "    ]\n",
    "    chunks = [\n",
    "        input_text[x[1] : y[1]]\n",
    "        for x, y in zip([(1, 0)] + chunk_positions[:-1], chunk_positions)\n",
    "    ]\n",
    "    span_annotations = [\n",
    "        (x[0], y[0]) for (x, y) in zip([(1, 0)] + chunk_positions[:-1], chunk_positions)\n",
    "    ]\n",
    "    return chunks, span_annotations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_text = \"Berlin is the capital and largest city of Germany, both by area and by population. Its more than 3.85 million inhabitants make it the European Union's most populous city, as measured by population within city limits. The city is also one of the states of Germany, and is the third smallest state in the country in terms of area.\"\n",
    "\n",
    "# determine chunks\n",
    "chunks, span_annotations = chunk_by_sentences(input_text, tokenizer)\n",
    "print('Chunks:\\n- \"' + '\"\\n- \"'.join(chunks) + '\"')\n",
    "\n",
    "# chunk afterwards (context-sensitive chunked pooling)\n",
    "inputs = tokenizer(input_text, return_tensors='pt')\n",
    "model_output = model(**inputs)\n",
    "embeddings = late_chunking(model_output, [span_annotations])[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## test implementation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/jan/miniconda3/envs/advanced_rag/lib/python3.11/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n",
      "Some weights of the model checkpoint at microsoft/table-transformer-structure-recognition were not used when initializing TableTransformerForObjectDetection: ['model.backbone.conv_encoder.model.layer2.0.downsample.1.num_batches_tracked', 'model.backbone.conv_encoder.model.layer3.0.downsample.1.num_batches_tracked', 'model.backbone.conv_encoder.model.layer4.0.downsample.1.num_batches_tracked']\n",
      "- This IS expected if you are initializing TableTransformerForObjectDetection from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing TableTransformerForObjectDetection from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "import os\n",
    "from dotenv import load_dotenv\n",
    "\n",
    "load_dotenv()\n",
    "\n",
    "# Get the current working directory\n",
    "current_dir = os.getcwd()\n",
    "\n",
    "# Assuming your notebook is in a subdirectory of the main project folder,\n",
    "# go up two levels to reach the project root\n",
    "project_root = os.path.abspath(os.path.join(current_dir, '..', '..', '..'))\n",
    "sys.path.append(project_root)\n",
    "\n",
    "\n",
    "from app.doc_processing import process_doc, ProcessDocConfig\n",
    "from app.vectorstore import get_chroma_store_as_retriever, add_docs_to_store\n",
    "import os\n",
    "\n",
    "pdf_path = '/Users/jan/Desktop/advanced_rag/dev_tests/test_data/el_nino.pdf'\n",
    "#pdf_path = '/Users/jan/Desktop/advanced_rag/dev_tests/test_data/attention_is_all.pdf'\n",
    "tag = \"attention\"\n",
    "table_processing = ['Header', 'Footer', 'Image', 'FigureCaption', 'Formula']\n",
    "config = ProcessDocConfig(\n",
    "        tag=tag,\n",
    "        filepath=pdf_path,\n",
    "        unwanted_categories_list=table_processing\n",
    "    )\n",
    "\n",
    "    # Process the document\n",
    "processed_docs = process_doc(config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from app.doc_processing.late_chunking import apply_late_chunking\n",
    "docs = apply_late_chunking(processed_docs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Creating new FAISS index\n",
      "New FAISS index saved to app/data/FAISS_STORE\n",
      "FAISS index saved to app/data/FAISS_STORE\n"
     ]
    }
   ],
   "source": [
    "from app.vectorstore.experimental import get_faiss_store_as_retriever, add_docs_to_faiss_store\n",
    "retriever = get_faiss_store_as_retriever()\n",
    "add_docs_to_faiss_store(retriever, docs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "attention\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/jan/miniconda3/envs/advanced_rag/lib/python3.11/site-packages/transformers/tokenization_utils_base.py:1601: FutureWarning: `clean_up_tokenization_spaces` was not set. It will be set to `True` by default. This behavior will be depracted in transformers v4.45, and will be then set to `False` by default. For more details check this issue: https://github.com/huggingface/transformers/issues/31884\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "from app.chat import create_RAG_output, get_result_docs, ChatConfig\n",
    "import app.llm\n",
    "\n",
    "config = ChatConfig(\n",
    "            tag=tag,\n",
    "            k=10,\n",
    "            llm=app.llm.get_groq_llm(),\n",
    "            expand_by_answer=False,\n",
    "            expand_by_mult_queries=False,\n",
    "            reranking=False,\n",
    "            use_bm25=False\n",
    "    )\n",
    "config.history_awareness(False)\n",
    "\n",
    "query = \"Which BLEU score did the transformer base-model achieve?\"\n",
    "query = \"what is el nino?\"\n",
    "result_docs, _ = get_result_docs(config, query, retriever=retriever)\n",
    "context = ''.join(result_docs)\n",
    "final_answer = create_RAG_output(context, query, config.llm)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'El Niño is a natural phenomenon in the tropical Pacific that influences weather around the globe. It causes changes in the jet stream that can point storms directly at California.'"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "final_answer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['The soaking storms will raise the ﬂood threat across much of California into next week, but it appears the wet pattern is likely to continue well into February as a more typical El Niño pattern kicks into gear.\\n\\nEl Niño – a natural phenomenon in the tropical Paciﬁc that inﬂuences weather around the globe – causes changes in the jet stream that can point storms directly at California. Storms can also tap into an extra-potent supply of moisture from the tropics called an atmospheric river.',\n",
       " 'A potent pair of atmospheric rivers will drench California as El Niño makes its ﬁrst mark on winter\\n\\nBy Mary Gilbert, CNN Meteorologist\\n\\nUpdated: 3:49 PM EST, Tue January 30, 2024\\n\\nSource: CNN\\n\\nA potent pair of atmospheric river-fueled storms are about to unleash a windy and incredibly wet week in California in what is the ﬁrst clear sign of the inﬂuence El Niño was expected to have on the state this winter.',\n",
       " 'El Niño hasn’t materialized many atmospheric rivers for California so far this winter, with most hitting the Paciﬁc Northwest.\\n\\nBut all that is set to change this week.\\n\\nThe ﬁrst of the two potent atmospheric river storms will develop and point at Northern California Wednesday before sweeping down the state and targeting Southern California Thursday.',\n",
       " 'Below average temperatures and above average precipitation amounts are likely to persist into mid-February, according to outlooks produced by the Climate Prediction Center.\\n\\nCNN’s Stephanie Elam contributed to this report\\n\\nSee Full Web Article']"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "result_docs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## retrieval test -> Berlin"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_span_annotations(docs, tokenizer):\n",
    "    \"\"\"\n",
    "    Generate span annotations for a list of documents.\n",
    "\n",
    "    Args:\n",
    "        docs (list): List of document objects.\n",
    "        tokenizer: The tokenizer to use for tokenizing document content.\n",
    "\n",
    "    Returns:\n",
    "        list: List of (start, end) tuples representing span annotations.\n",
    "    \"\"\"\n",
    "    span_annotations = []\n",
    "    start = 0\n",
    "    for doc in docs:\n",
    "        doc_tokens = tokenizer(doc.page_content, return_tensors='pt', add_special_tokens=False)\n",
    "        end = start + len(doc_tokens['input_ids'][0])\n",
    "        span_annotations.append((start, end))\n",
    "        start = end\n",
    "    return span_annotations\n",
    "\n",
    "def chunk_by_sentences(input_text: str, tokenizer: callable):\n",
    "    inputs = tokenizer(input_text, return_tensors='pt', return_offsets_mapping=True)\n",
    "    punctuation_mark_id = tokenizer.convert_tokens_to_ids('.')\n",
    "    sep_id = tokenizer.convert_tokens_to_ids('[SEP]')\n",
    "    token_offsets = inputs['offset_mapping'][0]\n",
    "    token_ids = inputs['input_ids'][0]\n",
    "    chunk_positions = [\n",
    "        (i, int(start + 1))\n",
    "        for i, (token_id, (start, end)) in enumerate(zip(token_ids, token_offsets))\n",
    "        if token_id == punctuation_mark_id\n",
    "        and (\n",
    "            token_offsets[i + 1][0] - token_offsets[i][1] > 0\n",
    "            or token_ids[i + 1] == sep_id\n",
    "        )\n",
    "    ]\n",
    "    chunks = [\n",
    "        input_text[x[1] : y[1]]\n",
    "        for x, y in zip([(1, 0)] + chunk_positions[:-1], chunk_positions)\n",
    "    ]\n",
    "    return chunks\n",
    "\n",
    "from langchain.schema import Document\n",
    "\n",
    "def add_sentences_to_chroma(retriever, sentences, tag):\n",
    "    docs = [Document(page_content=sentence, metadata={\"tag\": tag}) for sentence in sentences]\n",
    "    retriever.vectorstore.add_documents(docs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from app.doc_processing.late_chunking import chunk_by_sentences, apply_late_chunking\n",
    "from app.vectorstore import add_sentences_to_chroma, get_chroma_store_as_retriever\n",
    "from app.vectorstore.experimental import get_faiss_store_as_retriever, add_late_chunked_docs_to_faiss\n",
    "from transformers import AutoTokenizer\n",
    "\n",
    "input_text = \"Berlin is the capital and largest city of Germany, both by area and by population. Its more than 3.85 million inhabitants make it the European Union's most populous city, as measured by population within city limits. The city is also one of the states of Germany, and is the third smallest state in the country in terms of area.\"\n",
    "\n",
    "def process_and_store_text(input_text: str, tag: str):\n",
    "    # Initialize tokenizer\n",
    "    tokenizer = AutoTokenizer.from_pretrained('jinaai/jina-embeddings-v2-small-en', trust_remote_code=True)\n",
    "\n",
    "    # Split into sentences\n",
    "    sentences = chunk_by_sentences(input_text, tokenizer)\n",
    "\n",
    "    # Store sentences in Chroma\n",
    "    chroma_retriever = get_chroma_store_as_retriever()\n",
    "    add_sentences_to_chroma(chroma_retriever, sentences, tag)\n",
    "\n",
    "    # Apply late chunking\n",
    "    late_chunked_docs = apply_late_chunking([Document(page_content=sentence) for sentence in sentences])\n",
    "\n",
    "    # Store late-chunked embeddings in FAISS\n",
    "    faiss_retriever = get_faiss_store_as_retriever()\n",
    "    add_late_chunked_docs_to_faiss(faiss_retriever, late_chunked_docs)\n",
    "\n",
    "    print(f\"Processed and stored {len(sentences)} sentences with tag '{tag}'\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/jan/miniconda3/envs/advanced_rag/lib/python3.11/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "from transformers import AutoTokenizer\n",
    "input_text = \"Berlin is the capital and largest city of Germany, both by area and by population. Its more than 3.85 million inhabitants make it the European Union's most populous city, as measured by population within city limits. The city is also one of the states of Germany, and is the third smallest state in the country in terms of area.\"\n",
    "tokenizer = AutoTokenizer.from_pretrained('jinaai/jina-embeddings-v2-small-en', trust_remote_code=True)\n",
    "\n",
    "sentences = chunk_by_sentences(input_text, tokenizer)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_chroma import Chroma\n",
    "import os\n",
    "from uuid import uuid4\n",
    "import json\n",
    "from langchain_community.vectorstores.utils import filter_complex_metadata\n",
    "from langchain_core.documents import Document\n",
    "from app.vectorstore.embeddings import get_ollama_embeddings, JinaEmbeddings\n",
    "from dotenv import load_dotenv\n",
    "\n",
    "load_dotenv()\n",
    "chroma_retriever = Chroma(persist_directory=os.getenv(\"CHROMA_PATH\"),\n",
    "                          embedding_function=JinaEmbeddings()).as_retriever()\n",
    "add_sentences_to_chroma(chroma_retriever, sentences, 'test')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Creating new FAISS index\n",
      "New FAISS index saved to app/data/FAISS_STORE\n",
      "FAISS index saved to app/data/FAISS_STORE\n"
     ]
    }
   ],
   "source": [
    "from app.vectorstore.experimental import get_faiss_store_as_retriever, add_docs_to_faiss_store\n",
    "from app.doc_processing.late_chunking import apply_late_chunking\n",
    "\n",
    "docs = [Document(page_content=sentence, metadata={\"tag\": 'test', 'source': 'Berlin.txt'}) for sentence in sentences]\n",
    "late_chunked_docs = apply_late_chunking(docs)\n",
    "\n",
    "faiss_retriever = get_faiss_store_as_retriever()\n",
    "add_docs_to_faiss_store(faiss_retriever, late_chunked_docs)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Number of requested results 5 is greater than number of elements in index 3, updating n_results = 3\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Chroma Results:\n",
      "Score: 57.7374, Content: Berlin is the capital and largest city of Germany, both by area and by population....\n",
      "Score: 63.2674, Content:  Its more than 3.85 million inhabitants make it the European Union's most populous city, as measured...\n",
      "Score: 63.5306, Content:  The city is also one of the states of Germany, and is the third smallest state in the country in te...\n",
      "\n",
      "FAISS Results:\n",
      "Score: 60.5736, Content:  The city is also one of the states of Germany, and is the third smallest state in the country in te...\n",
      "Score: 61.0898, Content:  Its more than 3.85 million inhabitants make it the European Union's most populous city, as measured...\n",
      "Score: 61.7908, Content: Berlin is the capital and largest city of Germany, both by area and by population....\n"
     ]
    }
   ],
   "source": [
    "query = \"capital\"\n",
    "\n",
    "# Perform similarity search with score\n",
    "chroma_results = chroma_retriever.vectorstore.similarity_search_with_score(query, k=5)\n",
    "faiss_results = faiss_retriever.vectorstore.similarity_search_with_score(query, k=5)\n",
    "\n",
    "# Print results\n",
    "print(\"Chroma Results:\")\n",
    "for doc, score in chroma_results:\n",
    "    print(f\"Score: {score:.4f}, Content: {doc.page_content[:100]}...\")\n",
    "\n",
    "print(\"\\nFAISS Results:\")\n",
    "for doc, score in faiss_results:\n",
    "    print(f\"Score: {score:.4f}, Content: {doc.page_content[:100]}...\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## test comparison on pdf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at microsoft/table-transformer-structure-recognition were not used when initializing TableTransformerForObjectDetection: ['model.backbone.conv_encoder.model.layer2.0.downsample.1.num_batches_tracked', 'model.backbone.conv_encoder.model.layer3.0.downsample.1.num_batches_tracked', 'model.backbone.conv_encoder.model.layer4.0.downsample.1.num_batches_tracked']\n",
      "- This IS expected if you are initializing TableTransformerForObjectDetection from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing TableTransformerForObjectDetection from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "import os\n",
    "from dotenv import load_dotenv\n",
    "\n",
    "load_dotenv()\n",
    "\n",
    "# Get the current working directory\n",
    "current_dir = os.getcwd()\n",
    "\n",
    "# Assuming your notebook is in a subdirectory of the main project folder,\n",
    "# go up two levels to reach the project root\n",
    "project_root = os.path.abspath(os.path.join(current_dir, '..', '..', '..'))\n",
    "sys.path.append(project_root)\n",
    "\n",
    "\n",
    "from app.doc_processing import process_doc, ProcessDocConfig\n",
    "from app.vectorstore import get_chroma_store_as_retriever, add_docs_to_store\n",
    "import os\n",
    "\n",
    "pdf_path = '/Users/jan/Desktop/advanced_rag/dev_tests/test_data/el_nino.pdf'\n",
    "#pdf_path = '/Users/jan/Desktop/advanced_rag/dev_tests/test_data/attention_is_all.pdf'\n",
    "tag = \"attention\"\n",
    "table_processing = ['Header', 'Footer', 'Image', 'FigureCaption', 'Formula']\n",
    "config = ProcessDocConfig(\n",
    "        tag=tag,\n",
    "        filepath=pdf_path,\n",
    "        unwanted_categories_list=table_processing\n",
    "    )\n",
    "\n",
    "    # Process the document\n",
    "processed_docs = process_doc(config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "from app.vectorstore import get_chroma_store_as_retriever, add_docs_to_store\n",
    "from app.vectorstore.embeddings import get_ollama_embeddings, JinaEmbeddings\n",
    "from dotenv import load_dotenv\n",
    "\n",
    "load_dotenv()\n",
    "\n",
    "# attention: uses ollama embeddings on default\n",
    "chroma_retriever = get_chroma_store_as_retriever(embeddings=JinaEmbeddings())\n",
    "add_docs_to_store(chroma_retriever, processed_docs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Creating new FAISS index\n",
      "New FAISS index saved to app/data/FAISS_STORE\n",
      "FAISS index saved to app/data/FAISS_STORE\n"
     ]
    }
   ],
   "source": [
    "from app.doc_processing.late_chunking import apply_late_chunking\n",
    "from app.vectorstore.experimental import get_faiss_store_as_retriever, add_docs_to_faiss_store\n",
    "\n",
    "docs = apply_late_chunking(processed_docs)\n",
    "faiss_retriever = get_faiss_store_as_retriever()\n",
    "add_docs_to_faiss_store(faiss_retriever, docs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Chroma Results:\n",
      "Score: 50.9058, Content: The storm will start out very warm – fueled by moisture from near Hawaii that earns it the moniker o...\n",
      "Score: 52.4925, Content: El Niño hasn’t materialized many atmospheric rivers for California so far this winter, with most hit...\n",
      "Score: 54.2004, Content: The snow is welcomed for California’s snowpack, which has been beleaguered by warmth and storms that...\n",
      "Score: 54.7581, Content: Go to the full CNN experience\n",
      "\n",
      "© 2024 Cable News Network. A Warner Bros. Discovery Company. All Righ...\n",
      "Score: 54.9169, Content: Showery weather will linger across much of California Friday as moisture slowly pushes out and acros...\n",
      "\n",
      "FAISS Results:\n",
      "Score: 57.9215, Content: A potent pair of atmospheric rivers will drench California as El Niño makes its ﬁrst mark on winter\n",
      "...\n",
      "Score: 68.2715, Content: El Niño hasn’t materialized many atmospheric rivers for California so far this winter, with most hit...\n",
      "Score: 72.3097, Content: The soaking storms will raise the ﬂood threat across much of California into next week, but it appea...\n",
      "Score: 73.2878, Content: The snow is welcomed for California’s snowpack, which has been beleaguered by warmth and storms that...\n",
      "Score: 73.8918, Content: The storm will start out very warm – fueled by moisture from near Hawaii that earns it the moniker o...\n"
     ]
    }
   ],
   "source": [
    "query = \"California\"\n",
    "\n",
    "# Perform similarity search with score\n",
    "chroma_results = chroma_retriever.vectorstore.similarity_search_with_score(query, k=5)\n",
    "faiss_results = faiss_retriever.vectorstore.similarity_search_with_score(query, k=5)\n",
    "\n",
    "# Print results\n",
    "print(\"Chroma Results:\")\n",
    "for doc, score in chroma_results:\n",
    "    print(f\"Score: {score:.4f}, Content: {doc.page_content[:100]}...\")\n",
    "\n",
    "print(\"\\nFAISS Results:\")\n",
    "for doc, score in faiss_results:\n",
    "    print(f\"Score: {score:.4f}, Content: {doc.page_content[:100]}...\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "advanced_rag",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
